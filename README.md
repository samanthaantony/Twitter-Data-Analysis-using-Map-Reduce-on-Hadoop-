## Twitter Data Analysis on Hadoop Map-Reduce


### Description

The purpose of this project is to develop a simple Map-Reduce program on Hadoop that analyzes data from twitter and improve the performance of the code using a combiner for the first Map Reduce job and using in mapper combining for the second map reduce job.

In this project, you are asked to implement a simple graph algorithm that needs two Map-Reduce jobs.We use real data from Twitter from 2010. The dataset represents the follower graph that contains links between tweeting users in the form:id,follower_id. For example,

12,13
12,14
12,15
16,17

Here, users 13,14 and 15 are followers of user 12, while user 17 is a follower of user 16.We have taken a small dataset which contains 10,000 lines of complete which is in small-twitter.csv.

First, for each twitter user you count the number of users she follows. Then, you group the users by their number of users they follow and for each group you count how many users belong to this group. That is, the result will have lines such as:

10 30

which says that there are 30 users who follow 10 users.

### Compile & Running

Compile Twitter.java using:

```java
run twitter.build
```

and you can run Twitter.java in standalone mode over a small dataset using:

```java
sbatch twitter.local.run
```

Results generated by the program will be in the directory output. These results should be:

```java

1	8527
2	563
3	72
4	21
5	3
6	3
7	2

```


The results generated by your program will be in the directory output. These results should be:

```java
sbatch twitter.distr.run
```

This will process the data on the large dataset which is on Comet and will write the result in the directory output-distr. These results should be similar to the results in the file large-solution.txt. 

Results generated by the program will be in the directory output-distr. 

##### Pseudo-Code

```java

// The first Map-Reduce is:

map ( key, line ):
  read 2 integers from the line into the variables id and follower_id(delimeter is comma ",")
  emit (follower_id,id)

reduce ( follower_id, ids ):
  count = 0
  for n in ids
      count++
  emit(follower_id,count)

// The second Map-Reduce is:

map ( key, line ):
  read 2 integers from the line into the variables follower_id and count(delimeter is tab "\t")
  emit(count,1)

reduce ( count, values ):
  sum = 0
  for v in values
      sum += v
  emit(count,sum)
```
In the first map reduce, the ids in the reducer is the sequence of all users followed by the user with follower_id.
Java main program, args[0] is the twitter file,args[1] is the intermediate directory(output of job1 and input of job2) and args[2] is the output directory. The input and output must be text formats
